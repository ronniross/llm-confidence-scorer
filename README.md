# LLM Confidence Scorer (beta v.0.1.4)

This repository presents a set of auxiliary systems designed to provide a measure of estimated confidence for the outputs generated by Large Language Models.

It seeks to address the tendency for LLMs to produce fluent and convincing text that may sometimes be factually incorrect or represent "hallucinations," without any explicit signal of uncertainty.

The goal of this system is to act as a valuable layer on top of an LLM integration, giving developers and users an indication of how much trust to place in a specific LLM response regarding a particular query or task.

> This project is licensed under the MIT License.
> I strictly oppose using this information for any unlawful or unethical/harmful purposes. I am not liable for any improper use of the information shared in this repository.

## Approaches

Here are the different approaches explored for confidence estimation:

1.  **Self-Evaluation (Using the Primary LLM)**
   
<p align="center">
  <img src=".github/part1.png" alt="Visualization Part 1: Self-Evaluation Concept" />
  <img src=".github/part2.png" alt="Visualization Part 2: Self-Evaluation Concept" />
</p>

 **Concept:** The primary LLM that generated the initial response is used again to estimate its own confidence. 
    
This can be achieved by crafting a subsequent prompt that asks the LLM to evaluate its previous answer, perhaps by asking it to justify its claims, rate its certainty on a scale, or answer a slightly rephrased question to check for consistency.

**Distinction:** This method is the most "self-contained" as it relies solely on the capabilities of the single primary LLM. It does not require a separate model or access to internal model states. 
    
Its confidence estimation comes from the model's ability to analyze and comment on its own output when prompted appropriately.

2.  **Confidence Estimation via a Second, Separate-cloned Model:**

<p align="center">
  <img src=".github/part3.png" alt="Visualization Part 3: Self-Evaluation Concept" />
  <img src=".github/part4.png" alt="Visualization Part 4: Self-Evaluation Concept" />
</p>

**Concept:** This involves employing a completely independent machine learning model whose specific task is to analyze the output of the primary LLM (and potentially the original input) and produce a confidence score or signal. 
    
This secondary model can be trained or fine-tuned specifically for this evaluation task, potentially becoming highly specialized in detecting patterns indicative of uncertainty, inconsistency, or potential factual errors in LLM-generated text.

**Distinction:** This method introduces a separate component with its own processing pipeline. The confidence estimation is decoupled from the primary generation process and performed by a model dedicated solely to evaluation. 
    
It does not require internal access to the primary LLM.

3.  **Confidence Estimation via Cross-Modal Verification / External APIs:**

<p align="center">
  <img src=".github/part5.png" alt="Visualization Part 5: Self-Evaluation Concept" />
  <img src=".github/part6.png" alt="Visualization Part 6: Self-Evaluation Concept" />
</p>

**Concept:** This approach leverages external tools, APIs, or specialized models (which might be unimodal or multimodal) to validate specific claims or information presented within the primary LLM's textual response.

**Distinction:** Unlike self-evaluation (Method 1) which relies on the LLM's internal knowledge, or a dedicated textual evaluator (Method 2) which analyzes linguistic patterns, this method focuses on external validation of the response's content against potentially different modalities or structured data sources. It doesn't require a separate model trained solely on LLM output evaluation, nor access to the primary LLM's internals (Method 4). Its effectiveness hinges on the ability to parse verifiable claims from the LLM response and the availability/accuracy of suitable external APIs or tools for verification.


4.  **Confidence Estimation via Dedicated Internal Attention Heads:**

<p align="center">
  <img src=".github/part7.png" alt="Visualization Part 7: Self-Evaluation Concept" />
  <img src=".github/part8.png" alt="Visualization Part 8: Self-Evaluation Concept" />
</p>

**Concept:** This is the most integrated approach, requiring access to the internal architecture of the primary LLM. The idea is to identify, interpret, or potentially train specific "attention heads" within the transformer model to capture signals related to the model's confidence or uncertainty as it generates each token of the output. The confidence estimate is then derived by analyzing the patterns or activations of these particular attention heads during the initial generation pass.
**Distinction:** Unlike the other methods, this approach doesn't involve additional inference calls after the initial output is generated or the use of a separate model. The confidence signal is intended to be a byproduct extracted directly from the primary model's internal workings during its forward pass. This method is heavily reliant on the specific model architecture and availability of internal states.

Each of these approaches offers a different trade-off in terms of implementation complexity, computational cost, required access to the models' weights.

Note on Implementation Approach:

Since each model of each corporation or engineer is really different and already comes with pre-disposed logics for loading, inferencing, and auxiliary systems, these proposals of mine work better. If I proposed an integration using, for example, PyTorch only or full Unsloth, which is amazing, but then it wouldn't be that broader.

So, if the community wants to contribute with examples in specific frameworks, fork this repo and add them to a designated folder that I will integrate eventually into the main branch.

Another aspect is, if some corporation liked some pipeline from this repo or other of mine, I can be sponsored to expand those ideas, if ethically aligned with my philosophical values.

I will also consider expanding if I notice this specific repository is eventually getting traction and or engagement.

## Check Out My Other Projects

Iâ€™m working on several projects that explore advanced AI systems and their capabilities. Here are two related toolkits and frameworks:

- **[Symbiotic Core Library](https://github.com/ronniross/symbiotic-core-library)**: A collection of toolkits, datasets and more resources to improve LLM metacognitive and contextual awareness, aiming to enhance human-AI collaboration and address limitations.

- **[Core AGI Protocol](https://github.com/ronniross/core-agi-protocol)**: A framework to study how AGI or ASI might emerge from decentralized systems and to help guide its development.

- **[Latent Memory Module](https://github.com/ronniross/latent-memory/)**: A set of auxiliary systems designed to provide a measure of estimated confidence for the outputs generated by Large Language Models.
